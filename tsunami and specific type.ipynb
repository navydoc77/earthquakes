{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import jsonify\n",
    "from flask import render_template\n",
    "from flask import request\n",
    "from flask import url_for\n",
    "from flask import redirect\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import neighbors\n",
    "\n",
    "import db_conn\n",
    "\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_earthquake_dict(r):\n",
    "    return {\n",
    "    \"magnitude\" : float(r[0]),\n",
    "    \"place\": r[1],\n",
    "    \"time\": int(r[2]),\n",
    "    \"timezone\": float(r[3]),\n",
    "    \"url\": r[4],\n",
    "    \"tsunami\" :  int(r[5]),\n",
    "    \"id\" : r[6],\n",
    "    \"specific_type\" :  r[7],\n",
    "    \"title\" :  r[8],\n",
    "    \"country\" : r[9],\n",
    "    \"lat\" : float(r[11]),\n",
    "    \"lng\" :  float(r[10]),\n",
    "    \"depth\" :  float(r[12])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = [db_conn.earthquakes.magnitude, db_conn.earthquakes.place, db_conn.earthquakes.time, db_conn.earthquakes.timezone, db_conn.earthquakes.url, db_conn.earthquakes.tsunami, db_conn.earthquakes.id, db_conn.earthquakes.specific_type, db_conn.earthquakes.title, db_conn.earthquakes.country_de, db_conn.earthquakes.lng, db_conn.earthquakes.lat, db_conn.earthquakes.depth]\n",
    "all_results = db_conn.session.query(*sel).all()\n",
    "\n",
    "all_earthquakes = []\n",
    "\n",
    "for r in all_results:\n",
    "    transformed_dict = create_earthquake_dict(r)\n",
    "    all_earthquakes.append(transformed_dict)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_earthquakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = [\"place\", \"time\", \"timezone\", \"url\",  \"id\", \"specific_type\", \"title\", \"country\"]\n",
    "knn_df = df.drop(DROP_COLUMNS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNeighborAnalysis(X, y):\n",
    "\n",
    "    ################ TRAIN TEST SPLIT ####################\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "\n",
    "    ################ K-NEAREST NEIGHBOR ####################\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    roc_data_arrays = []\n",
    "    confusion_matrix_arrays = []\n",
    "\n",
    "    # try n_neighbors from 1 to 10\n",
    "    neighbors_settings = range(1, 11)\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # Instantiates the KNeighbor Model\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        # Fits the model to the training data\n",
    "        clf.fit(X_train, y_train)\n",
    "        # record training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "        # record generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "        # Compute Receiver operating characteristic (ROC)\n",
    "        y_scores = clf.predict_proba(X_test)\n",
    "        fpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\n",
    "        roc_data_arrays.append([fpr, tpr])\n",
    "\n",
    "        # Computes the tn, fp, fn, tp in confusion matrix\n",
    "        cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        confusion_matrix_arrays.append([tn, fp, fn, tp])\n",
    "\n",
    "    knn_annalysis_dict = {\n",
    "        \"x\" : [1,2,3,4,5,6,7,8,9,10],\n",
    "        \"training_scores\": training_accuracy,\n",
    "        \"test_scores\": test_accuracy,\n",
    "        \"roc_data_arrays\": roc_data_arrays,\n",
    "        \"confusion_matrix_arrays\": confusion_matrix_arrays\n",
    "        }\n",
    "\n",
    "    return (knn_annalysis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning():\n",
    "    ################# Lat \"Lng Depth Magnitude \"###############\n",
    "    # CASE 1: ALL Subfeatures included: Lng Depth Magnitude\n",
    "    # CASE 1: ALL CHECKED OFF\n",
    "    # CASE 1: PREFIX DESIGNATION: All\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    # NONE\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = knn_df[\"tsunami\"].values\n",
    "    X = knn_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    all_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat \"Lng Depth\" ###############\n",
    "    # CASE 2: LNG, DEPTH CHECKED OFF\n",
    "    # CASE 2: MAGNITUDE NOT CHECKED OFF\n",
    "    # CASE 2: PREFIX DESIGNATION: lng_depth_df\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    CASE2_DROP_COLUMNS = [\"magnitude\"]\n",
    "    lng_depth_df = knn_df.drop(CASE2_DROP_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = lng_depth_df[\"tsunami\"].values\n",
    "    X = lng_depth_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    lng_depth_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat \"Lng Magnitude\" ###############\n",
    "    # CASE 3: LNG AND MAGNITUDE CHECKED OFF\n",
    "    # CASE 3: DEPTH NOT CHECKED OFF\n",
    "    # CASE 3: PREFIX DESIGNATION: lng_magnitude\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    CASE3_DROP_COLUMNS = [\"depth\"]\n",
    "    lng_magnitude_df = knn_df.drop(CASE3_DROP_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = lng_magnitude_df[\"tsunami\"].values\n",
    "    X = lng_magnitude_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    lng_magnitude_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat \"Depth Magnitude\" ###############\n",
    "    # CASE 4: DEPTH AND MAGNITUDE CHECKED OFF\n",
    "    # CASE 4: LNG NOT CHECKED OFF\n",
    "    # CASE 4: PREFIX DESIGNATION: depth_magnitude\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    CASE4_DROP_COLUMNS = [\"lng\"]\n",
    "    depth_magnitude_df = knn_df.drop(CASE4_DROP_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = depth_magnitude_df[\"tsunami\"].values\n",
    "    X = depth_magnitude_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    depth_magnitude_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat \"Lng\" ###############\n",
    "    # CASE 5: LNG CHECKED OFF\n",
    "    # CASE 5: MAGNITUDE AND DEPTH NOT CHECKED OFF\n",
    "    # CASE 5: PREFIX DESIGNATION: lng_df\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    CASE5_DROP_COLUMNS = [\"magnitude\", \"depth\"]\n",
    "    lng_df = knn_df.drop(CASE5_DROP_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = lng_df[\"tsunami\"].values\n",
    "    X = lng_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    lng_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat \"Depth\" ###############\n",
    "    # CASE 6: DEPTH CHECKED OFF\n",
    "    # CASE 6: LNG AND MAG NOT CHECKED OFF\n",
    "    # CASE 6: PREFIX DESIGNATION: depth\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    CASE6_DROP_COLUMNS = [\"magnitude\", \"lng\"]\n",
    "    depth_df = knn_df.drop(CASE6_DROP_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = depth_df[\"tsunami\"].values\n",
    "    X = depth_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    depth_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat \"Magnitude\" ###############\n",
    "    # CASE 7: MAGNITUDE CHECKED OFF\n",
    "    # CASE 7: LNG DEPTH NOT CHECKED OFF\n",
    "    # CASE 7: PREFIX DESIGNATION: magnitude\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    CASE5_DROP_COLUMNS = [\"depth\", \"lng\"]\n",
    "    magnitude_df = knn_df.drop(CASE5_DROP_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = magnitude_df[\"tsunami\"].values\n",
    "    X = magnitude_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    magnitude_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "    ################# Lat ###############\n",
    "    # CASE 8: NO FEATURES SELECTED \n",
    "    # CASE 8: ALL BOXED UNCHECKED\n",
    "    # CASE 8: PREFIX DESIGNATION: lat_df\n",
    "\n",
    "    # Step 1: Drop columns\n",
    "    DROP_NEW_COLUMNS = [\"magnitude\", \"depth\", \"lng\"]\n",
    "    lat_df = knn_df.drop(DROP_NEW_COLUMNS, axis = 1)\n",
    "\n",
    "    # Step 2: Assign X and y values\n",
    "    y = lat_df[\"tsunami\"].values\n",
    "    X = lat_df.drop('tsunami', axis=1).values\n",
    "\n",
    "    # Step 3: Conducted Analysis and store reust in variable\n",
    "    lat_data = kNeighborAnalysis(X,y)\n",
    "\n",
    "\n",
    "    ################# RETURNING ALL CASE RESULTS #########################\n",
    "    all_knn_analysis_data = {\n",
    "        \"case1\" : all_data,\n",
    "        \"case2\" : lng_depth_data,\n",
    "        \"case3\" : lng_magnitude_data,\n",
    "        \"case4\" : depth_magnitude_data,\n",
    "        \"case5\" : lng_data,\n",
    "        \"case6\" : depth_data, \n",
    "        \"case7\" : magnitude_data,\n",
    "        \"case8\" : lat_data\n",
    "    }\n",
    "\n",
    "    return jsonify(all_knn_analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = knn_df[\"tsunami\"].values\n",
    "X = knn_df.drop('tsunami', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tsunami magnitude lat and lng\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "confusion_matrix_arrays = []\n",
    "roc_data_arrays = []\n",
    "\n",
    "# try n_neighbors from 1 to 10\n",
    "neighbors_settings = range(1, 11)\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "\n",
    "    # build the KNeighbor model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "    # fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    # Compute Receiver operating characteristic (ROC)\n",
    "    y_scores = clf.predict_proba(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\n",
    "    roc_data_arrays.append([fpr, tpr, threshold])\n",
    "    \n",
    "    # confusion matrix\n",
    "    # y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    confusion_matrix_arrays.append([tn, fp, fn, tp])\n",
    "    \n",
    "\n",
    "    \n",
    "# plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "# plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"n_neighbors\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data_arrays[9][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix_list[0]\n",
    "tn, fp, fn, tp = confusion_matrix_list[0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tn, fp, fn, tp)\n",
    "values = [tn, fp, fn, tp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = [\"ids\", \"geometry\", \"place\", \"url\", \"country\", \"specific_type\", \"time\", \"timezone\"]\n",
    "plot2 = df.drop(DROP_COLUMNS, axis = 1)\n",
    "plot2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "y = plot2[\"tsunami\"].values\n",
    "X = plot2.drop(['tsunami'], axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive rate\")\n",
    "plt.title(\"Logistic Regression ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = [\"ids\", \"geometry\", \"place\", \"url\", \"country\", \"specific_type\", \"time\", \"timezone\", \"lat\", \"lng\"]\n",
    "plot3 = df.drop(DROP_COLUMNS, axis = 1)\n",
    "plot3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "y = plot3[\"tsunami\"].values\n",
    "X = plot3.drop(['tsunami'], axis=1).values\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = [\"ids\", \"geometry\", \"place\", \"url\", \"country\", \"specific_type\", \"time\", \"timezone\", \"lat\", \"lng\"]\n",
    "plot4 = df.drop(DROP_COLUMNS, axis = 1)\n",
    "\n",
    "y = plot4[\"tsunami\"].values\n",
    "X = plot4.drop(['tsunami'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "\n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_log_loss(0.996,0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP_COLUMNS = [\"ids\", \"geometry\", \"place\", \"url\", \"country\", \"specific_type\", \"timezone\", \"time\", \"depth\"]\n",
    "# tsnuma_mag = df.drop(DROP_COLUMNS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dummy = DummyClassifier().fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_test)\n",
    "print(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\n",
    "\n",
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "print(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    # the fit method returns the object self, so we can instantiate\n",
    "    # and fit in one line\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    "    ax.set_xlabel(\"feature 0\")\n",
    "    ax.set_ylabel(\"feature 1\")\n",
    "axes[0].legend(loc=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
